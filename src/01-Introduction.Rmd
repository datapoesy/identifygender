---
title: "01-Introduction"
output:
  pdf_document: default
  html_document: default
  footer: "Copyright (c) 2017 Sreekumar Pillai"
---
\fontsize{11}{22}

```{r setup, include=FALSE}
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

# Introduction {#introduction}

Within the growth span of any organization, multiple systems get harnessed into the IT landscape.  Each of these systems tend to keep a version of the organizational data giving rise to concerns about its authenticity.  For any information, there should only be a single version of truth; at the same time, this remains in stark contrast with the organic, haphzard manner in which organizations grow.  

In this analysis while we look at customer records from a linkage context, the concept and solution is applicable to a wide range of scenarios and problems.  

##Objective

The objective of this controlled experiment is to:  

* Identify a practical solution for deduplication
* Set up a domain agnostic, reproducible experimental framework
* Test alternate deduplication solutions employing Machine Learning employing Supervised & Unsupervised methods
* Validate the output by comparing the input to the output
* Report the accuracy of the solution in quantitative terms

##Experiment Framework:

The experiment framework is illustrated in the image that follows:  


<span><center>
!["The Experiment Framework"](image/experiment-process.png){width=120% }</center>
<h4><center>Figure.1.1 The Experiment Framework</center></h4> </span>



##Limitations:

The solution presented herein is based on simulated data.  The practical efficiency of the solution depends on the extent to which the dataset can be made to reflect the poor quality of real world data.  

While the solution is defined, interpreted and solved in R, practical implementation is considered to be beyond the scope of the current work.  R, being an in-memory statistical platform will find it challenging to scale up to high-volume of enterprise datasets.  

The framework and solution would need to be re-programmed to meet the performance demands of high-volume datasets: sources exceeding millions of records.  

##Acknowledgements:
The study has been done using the open source statistical tool R [@R-base], R Studio, Forecast [@Fcast2017],[@Fcast2016], and other packages.  
The project makes use of the R Project Template [@R-ProjectTemplate] for statistical configurations.  Reports are generated using Knitr, [@R-knitr] Bookdown [@R-bookdown], and R Markdown @R-rmarkdown packages.  Illustrations make extensive use of the ggplot2 [@R-ggplot2] package.  
Test data was generated using the Laravel framework[@Laravel2017] & Postgresql [@Postgres2017].  

Citations acknowledge credit to the owners of the respective tools and packages:



