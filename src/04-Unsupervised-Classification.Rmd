---
title: "04-Unsupervised Classifications"
output:
  pdf_document: default
  html_document: default
---


#Unsupervised Machine Learning:

Potential matching pairs are identified based on weights.  Each record in a comparative pair gets a weight based on its proximity to its twin, in turn based on the similarity of values in their variables.  Values above a certain threshold is considered to be an identical pair.  The following is an output of the weighing process:

The weight distribution shows the number of records within the given range.  For instance, there are 45029 records within the weight range of [0.05,0.1]

##Assigning Weights for classification


Identification of matching using weights is based on computing weight thresholds.  Most commonly, this is achieved manually.  The following is the output of the classification by weight thresholds:

***

```{r chapter 4-1, echo=FALSE, message=FALSE, warning=FALSE}
#result <- epiClassify(rpairs.00, 0.85,0.40)
suppressPackageStartupMessages(library(pander))
panderOptions("table.split.table", Inf)  ## don't split tables
pander(summary(results.11)[1:2])

```


The difference between the duplicates created and the identified linkages denotes the error from the deduplication process.

###Unsupervised classification using weights

It is possible to use an unsupervised classification on the weighted pairs to identify the optimal pairs.  The following table gives the output of an unsupervised classification.

**The engine identifies 3 times the original duplicate number.**

```{r Chapter 4-2, echo=FALSE, message=FALSE, warning=FALSE}
panderOptions('table.alignment.default','left')

read.cl.results.11 <- read.csv (file="cl.results.11.csv")
rownames(read.cl.results.11) <- NULL
read.cl.results.11 <- read.cl.results.11[,-c(1,6,10)]
colnames(read.cl.results.11) <- c("No.","Total\n.Records","Duplicates","Non\n.Duplicates","Quality\n.Perc","Ident\n.Pairs","Pairs\n.Perc")

pander(read.cl.results.11, caption="Unsupervised Classification Results", short= TRUE, split.tables=200, keep.line.breaks = TRUE)

```


###Manual  Classification using thresholds

It is possible to classify the records using thresholds identified manually.  In the controlled experiment, the following thresholds give exactly the same value as unsupervised classification:  

Higher value = 1.0 (Anything beyond this does not impact the classification output)   
Lower value  = 0.85 (This value determines whether the classification is optimistic or pessimistic)   

The number of pairs between these threshold values are `r results.11.dupli.count`

The number of False positives using this approach is 896 records.  Since the two thresholds determine the boundaries of match, there is a possibility of false positives lying between this range and being identified as positive matches.

###Unsupervised classification using optimal thresholds

It is possible to use a classification function to determine an optimal threshold from a trained set and use that as the classification threshold.  This approach also does not provide adequate accuracy.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
panderOptions('table.alignment.default','left')

read.cl.results.33 <- read.csv (file="cl.results.33.csv")

rownames(read.cl.results.33) <- NULL
read.cl.results.33 <- read.cl.results.33[,-c(1,6,10)]
colnames(read.cl.results.33) <- c("No.","Total\n.Records","Duplicates","Non\n.Duplicates","Quality\n.Perc","Ident\n.Pairs","Pairs\n.Perc")
pander(read.cl.results.33, caption="Unsupervised Classification Results - Optimal Threshold", short= TRUE, split.tables=200 ,keep.line.breaks = TRUE)

```


###Pessimistic Thresholds
It is advised to adopt a pessimistic approach while identifying threshold values since probable ones could be verified clerically to avoid false positives:  

At the same time, a highly pessimistic approach will result in a high number of clerical verifications which becomes impractical. 

###Supervised Classification

The next chapter addresses these challenges using supervised classification with labeled examples.  Results from other classification methods are ignored in this report since their output has not been satisfactory.
