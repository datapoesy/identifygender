---
title: "06-Supervised Calssfication with Trained Data"
output:
  pdf_document: default
  html_document: default
---


#Supervised Classification with large datasets

This chapter details results from multiple iterations of the dataset with varied record counts and quality.  

###Experiment 1 
* Record Count - 5000, Training set - 503 pairs, Quality Level - 53.35% to 91.22%

The table reports the accuracy of the deduplication engine.  Note that the quality of data is proportional to the Total records.  This is due to the addition of duplicate and non-duplicate records to the base dataset.

```{r chapter 6-1, echo=FALSE, message=FALSE, warning=FALSE}
results.all.exp <- read.csv(file="results/results.all.exp.csv")
results.all.exp <- results.all.exp[,-c(1)]
results.iter1 <- results.all.exp[c(1:5),]
colnames(results.iter1) <- c("No.","Total\n.Records","Duplicates","Non\n.Duplicates","Record\n.Pairs","Quality\n.Perc","Ident\n.Pairs","Accuracy","Error")
panderOptions('table.alignment.default','left')
pander(results.iter1, caption="Results from experiment on 5K records", short= TRUE, split.tables=180, keep.line.breaks = TRUE)

```

###All Experiments
The table below summarises results from 6 iterations of experiments starting with 5K to 30K records.  

* Record Count - In increments of 5K
* Training set - 500 pairs
* Quality Level - 53.35% to 91.22%

```{r chapter 6-2, echo=FALSE, message=FALSE, warning=FALSE}

results.iter0 <- results.all.exp
rownames(results.iter0) <- NULL
colnames(results.iter0) <- c("No.","Total\n.Records","Duplicates","Non\n.Duplicates","Record\n.Pairs","Quality\n.Perc","Ident\n.Pairs","Accuracy","Error")
panderOptions('table.alignment.default','left')
pander(results.iter0, caption="Results from Supervised classification iterations", short= TRUE, split.tables=180,keep.line.breaks = TRUE)

```



##Total Records Vs. Error

The following graph plots the the Error against the total number of records:  


```{r, echo=FALSE, fig.cap="Total Records Vs. Error", fig.height=2.8, fig.width=5, message=FALSE, warning=FALSE}
results.ggplot <- results.all.exp[,-c(1,3,5)]

p <- ggplot(data=results.ggplot, aes(x=Total.Records, y=Error, group=1, xlab="test")) +
  geom_line(color="blue")+
  geom_point() +
  labs(x = "Total Records") +
  labs(y = "Error in %")
p
```

* The portion below the zero line is the optimistic half.  The deduplication engine has identified more pairs of records than the originally duplicated.  

* Till around 35K records, the deduplication engine is optimistic.
* Beyond 35K records, the engine is pessimistic.  
* This is a welcome trend since False positive records will be minimised.

###Quality Vs. Accuracy

The graph plots the Error percentage for 6 iterations from 5k to 30k records for 5 levels of quality : 53.35%, 62.93%, 74.45%, 84.35% and 91.22%; starting from the most corrupted dataset to the highest quality represented by the figure 91.22.

```{r, echo=FALSE, fig.cap="Quality Vs.Error", fig.height=2.8, fig.width=5, message=FALSE, warning=FALSE}
results.ggplot <- results.all.exp[,-c(1,3,5)]

p <- ggplot(data=results.ggplot, aes(x=Quality.Perc, y=Error, group=1, xlab="test")) +
  geom_line(linetype = "dashed", color="blue")+
  geom_point()+
  labs(x = "Quality in %")+
  labs(y = "Error in %")
p
```



As in the previous case, the error diminishes as the quality of the dataset increases, and the deduplication engine tends to take a pessimistic approach to match the pairs.